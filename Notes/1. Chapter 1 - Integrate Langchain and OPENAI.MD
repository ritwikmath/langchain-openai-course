
Langchain uses LCEL or LangChain expression language, a declarative way, to compose chains togather.
prompt, LLM model and output parser can be chained together to create a chain.

```python
# Import the load_dotenv function to load environment variables from a .env file
from dotenv import load_dotenv  
# Import the OpenAI model from the langchain_openai library
from langchain_openai import OpenAI  
# Import PromptTemplate and ChatPromptTemplate from the langchain_core.prompts module
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate  
# Import StrOutputParser from the langchain_core.output_parsers module
from langchain_core.output_parsers import StrOutputParser  

# Load environment variables from the .env file
load_dotenv()  

# Initialize the OpenAI model
llm = OpenAI()  

# Define a ChatPromptTemplate using a template string
# This template will ask how to say a given input in a specified language
prompt = ChatPromptTemplate.from_template("How to say {input} in language {language}?")  

# Initialize a string output parser to handle the model's response
output_parser = StrOutputParser()  

# Create a chain that combines the prompt, the model, and the output parser
chain = prompt | llm | output_parser

# Invoke the chain with a specific input and language
response = chain.invoke({  
    "input": "How many states are there in USA?",  
    "language": "German"  
})  

# Print the response, which should be the translation of the input question into German
print(response)
```


# Prompt and Prompt Template

A prompt is an input that we provide to an AI Model to modify or guide the model behavior, direct the model toward a specific task and get a particular response.

**Example:**

```plaintext

Write a query for the table `employees` to find all rows where `department` equals `HR`.

```
 
 A prompt template is a framework that uses a reusable structure and placeholders for dynamic content. 

**Prompt Template:**

```plaintext

Using the schema below, write a SQL query to answer the question.

Schema:
{{schema}}

Question:
{{user_question}}

Output:
Provide the SQL query.

```

When a prompt template is populated with actual data and formatted to be used in an LLM, it becomes a `prompt`.

**Prompt:**

```plaintext

Using the schema below, write a SQL query to answer the question.

Schema:
CREATE TABLE employees (id INT, name TEXT, salary INT);

Question:
Retrieve all employees with a salary greater than 50000.

Output:
SELECT * FROM employees WHERE salary > 50000;

```

# Output Parser

Output parsers are responsible for formatting the output of an LLM model into a more suitable structure. There are many types of output parsers.

**Example - Json Output Parser:**

```python
# Import the load_dotenv function to load environment variables from a .env file
from dotenv import load_dotenv  
# Load environment variables from the .env file
load_dotenv()  

# Import necessary components from the langchain_core and langchain_openai libraries
from langchain_core.output_parsers import JsonOutputParser  
from langchain_core.prompts import PromptTemplate  
from langchain_core.pydantic_v1 import BaseModel, Field, validator  
from langchain_openai import OpenAI  

# Initialize the OpenAI model with a temperature of 0.0 (deterministic output)
model = OpenAI(temperature=0.0)  

# Define a Pydantic model to represent the structure of a joke
class Joke(BaseModel):  
    # Define the 'setup' field with a description
    setup: str = Field(description="question to set up a joke")  
    # Define the 'punchline' field with a description
    punchline: str = Field(description="answer to resolve the joke")  

    # Add custom validation to ensure the 'setup' field ends with a question mark
    @validator("setup")  
    def question_ends_with_question_mark(cls, field):  
        if field[-1] != "?":  
            raise ValueError("Badly formed question!")  
        return field  

# Set up a JSON output parser that uses the Joke model to parse the output
parser = JsonOutputParser(pydantic_object=Joke)  

# Define a prompt template that includes instructions for the model
prompt_template = PromptTemplate(  
    template="Answer the user query.\n{format_instructions}\n{query}\n",  
    input_variables=["query"],  
    partial_variables={"format_instructions": parser.get_format_instructions()},  
)  

# Define the query to ask the model to generate a joke
query = "Tell me a joke."  

# Create a chain that combines the prompt, the model, and the parser
chain = prompt_template | model | parser  

# Invoke the chain with the query and get the output
output = chain.invoke({"query": query})  

# Print the output, which should be a Joke object parsed from the model's response
print(output)
```

Prompt can also be created following way

```python
# Define a prompt template that includes instructions for the model
prompt = PromptTemplate.from_template("Answer the user query.\n{format_instructions}\n{query}\n")

# Define the query to ask the model to generate a joke
query = "Tell me a joke."

# Create a chain that combines the prompt, the model, and the parser
chain = prompt | model | parser

# Invoke the chain with the query and format instructions, and get the output
output = chain.invoke({"query": query, "format_instructions": parser.get_format_instructions()})

# Print the output, which should be a Joke object parsed from the model's response
print(output)
```

**Output:**

```json

{
	'setup': 'Why did the tomato turn red?',
	'punchline': 'Because it saw the salad dressing!'
}

```

Partially formatted prompt templates are new prompt template that has subset of dynamic values and expect rest of the remaining values. 

```python

prompt = PromptTemplate(  
    template="Answer the user query.\n{format_instructions}\n{query}\n",  
    input_variables=["query"],  
    partial_variables={"format_instructions": parser.get_format_instructions()},  
)

```
